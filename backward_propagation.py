import numpy as np
"""
The implementation of the linear part of the backward propagation process for a single layer

Input:
dZ - The derivative of the cos function with respect to Z
cache - A tuple of (A_prev,W,b). A_prev is the A values of the previous layer. W - The weight values of the current
layer, The b value of the current layer
"""
def Linear_backward(dZ, cache):
    A_prev, W, b = cache
    m = A_prev.shape[1] #number of samples

    dA_prev = np.dot(W.T,dZ)

    dW = 1./m * np.dot(dZ,A_prev.T)
    db = 1./m * np.sum(dZ,axis=1,keepdims=True)

    return dA_prev,dW,db

"""
The implementation for the LINEAR -> ACTIVATION layer. The function first computed dZ and then applies the 
linear_backward function

Input:
dA - The derivative of the cos function with respect to A
cache - A tuple of (A_prev,W,b). A_prev is the A values of the previous layer. W - The weight values of the current
layer, The b value of the current layer
Activation a tuple of ('activation function name' and the activation_cache), activation_cache - contains z

"""
def linear_activation_backward(dA,cache,activation = "RELU"):

    linear_cache, activation_cache = cache
    # Relu activatio
    if activation == "RELU":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = Linear_backward(dZ, linear_cache)

    # Softmax activation
    elif activation == "SOFTMAX":
        dZ = softmax_backward(dA, activation_cache)
        dA_prev, dW, db = Linear_backward(dZ, linear_cache)
    else:
        return "ERROR"

    return dA_prev, dW, db

"""
This function implements backward propagation for a RELU unit


"""
def relu_backward(dA,activation_cache):

    Z = activation_cache
    dZ = np.array(dA, copy=True)
    dZ[Z <= 0] = 0

    return dZ



"""
This function implements backward propagation for a RELU unit
"""
def softmax_backward(dA,activation_cache):
    return "not implemented"

"""
Implement the backward propagation process for the entire network.

Inputs:
AL - the probabilities vector, the output of the forward propagation (L_model_forward)
Y - the true labels vector (the "ground truth" - true classifications)
Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache

"""
def L_model_backward(AL, Y, caches):
    # The dictionary with the gradients
    grads = {}

    m = AL.shape[1]
    num_of_layers = len(caches)  # Number of layers
    Y = Y.reshape(AL.shape)

    # The dA for the last layer
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    #The last layer uses softmax
    grads["dA" + str(num_of_layers-1)], grads["dW" + str(num_of_layers-1)], grads["db" + str(num_of_layers-1)] = \
        linear_activation_backward(dAL, caches[num_of_layers-1], activation="SOFTMAX")


    #Relu layers
    #From the last layer to the first
    for i in reversed(range(num_of_layers-1)):

        current_cache = caches[i]
        grads["dA" + str(i)], grads["dW" + str(i)], grads[
            "db" + str(i)] = linear_activation_backward(grads["dA" + str(i + 1)], current_cache)
    return  grads


"""
This function will update the parameters using gradient descent 

parameters – a python dictionary containing the DNN architecture’s parameters
grads – a python dictionary containing the gradients (generated by L_model_backward)
learning_rate – the learning rate used to update the parameters (the “alpha”)

"""
def update_parameters(parameters, grads, learning_rate):

    # Assuming parameters contains 2 parameters for each layers. W and b
    L = len(parameters) / 2

    #Gradient step
    for l in range(L):
        parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - learning_rate * grads["dW" + str(l + 1)]
        parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)]


    return parameters



